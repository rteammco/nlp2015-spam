#!/bin/bash

# A script that calls all of the other scripts in order to do the all of
# the preprocessing and model generation steps.
#!/bin/bash
#
# Make sure all of the individual scripts work before using this one.
#
# Steps are enumerated according the REAMDE.md.


setup() {
    # 1, 2. Download all the stuff (you have to do this manually).

    # 3. Make all of the necessary directories if they don't exist.
    # TODO - data split!
    mkdir -p Data/NGramTrain
    mkdir -p Data/NGramTest/lower_chars
    mkdir -p Data/NGramTest/lower_words
    mkdir -p Data/NGramTest/upper_chars
    mkdir -p Data/NGramTest/upper_words
    mkdir -p Data/NGramTrain/lower_chars
    mkdir -p Data/NGramTrain/lower_words
    mkdir -p Data/NGramTrain/upper_chars
    mkdir -p Data/NGramTrain/upper_words
    mkdir -p Models/Evaluations
}


bag_of_words_preprocess() {
    # 1. Preprocess first 67877 (90%) emails for filtered training bag-of-words set.
    echo "Generating bulk training data."
    python preprocess.py trec07p 1 67877 Data/train_bulk.arff -stopwords stopwords.txt

    # 2. Preprocess last 7542 (10%) emails for filtered testing bag-of-words set.
    echo "Generating bulk testing data."
    python preprocess.py trec07p 67878 75419 Data/test_bulk.arff -stopwords stopwords.txt
    
    # 3. Convert batch sets to standardized .arff files for Weka.
    echo "Converting bulk data to standardized data."
    bash convert

    # TODO - more stuff
}


ngram_preprocess() {
    # 1. Generate all of the N-Gram data files.
    echo "Generating all N-Gram data files."
    bash generate_ngram_models
    
    # 2. Build the N-Gram models using Berkley LM.
    echo "Training (building) the N-Gram models."
    bash build_ngram_models 3
    bash build_ngram_models 4 lower_chars upper_chars
    bash build_ngram_models 5 lower_chars upper_chars

    # 3. Evaluate all messages on all appropriate models: TODO (also update readme)

    # 4. Build arff from evaluated values: TODO (also update readme)

    # TODO - more stuff
}
