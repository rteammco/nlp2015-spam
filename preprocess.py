# Python script to convert the raw email data into .arff format for Weka.

import sys
import re
import email


# Fill this list using load_stopwords() function.
STOPWORDS = []

# Regex to match any number (int or float) within a string.
FLOAT_REGEX = r'[+-]?(\d+(\.\d*)?|\.\d+)([eE][+-]?\d+)?'

# Range of allowable ASCII characters (rest are ignored).
ASCII_MIN = 32
ASCII_MAX = 126

# Alls symbols to be ignored by the filters.
SYMBOLS = [
    '~', '`', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '+',
    '=', '{', '}', '[', ']', '\\', '|', ':', ';', '"', '\'', ',', '.', '?', '/'
]


def filter_words(words):
    """
    Removes all unwanted symbols from each word, and splits words around
    unwanted delimiters. Also removes digits. After all filtering is done,
    stopwords are also removed (assuming stopwords list is filled).
    NOTE: The words are NOT converted into lowercase in the final output.
    This is a word-by-word preprocessing step.
    """
    filtered = []
    for word in words:
        # Replace digits with arbitrary symbol, and split around it later
        word = re.sub(FLOAT_REGEX, SYMBOLS[0], word)
        split = False
        for symbol in SYMBOLS:
            if symbol in word:
                parts = word.split(symbol)
                words.extend(parts)
                split = True
                break
        if (not split) and (len(word) > 0) and (word.lower() not in STOPWORDS):
            filtered.append(word)
    return filtered


def process_text(text):
    """
    Processes a single string of text and returns an untagged version of it.
    That is, removes any HTML tags, and any content contained inside the tags,
    and returns the string as raw words.
    This is a character-by-character preprocessing step that also calls the
    word-for-word preprocessing.
    """
    # remove HTML tags and non-ascii characters:
    words = text.split()
    intag = False
    raw_words = []
    for word in words:
        untagged = ''
        for ch in word:
            if ch == '<':
                intag = True
            elif ch == '>':
                intag = False
            elif (not intag) and (ASCII_MIN <= ord(ch) <= ASCII_MAX):
                untagged += ch
        if len(untagged) > 0:
            raw_words.append(untagged)
    # remove stopwords, numbers, and symbols:
    raw_words = filter_words(raw_words)
    raw_text = ' '.join(raw_words)
    return raw_text


def process_multipart(part):
    """Recursively processes a part of the message body content."""
    if type(part) is str:
        return part
    maintype = part.get_content_maintype()
    if maintype == 'text':
        return part.get_payload()
    elif maintype == 'multipart':
        text = ''
        for sub_part in part.get_payload():
            text += process_multipart(sub_part)
        return text
    else:
        return ''


def process_message(mime_file):
    """
    Separately processes the email stored in the provided MIME file, and
    returns the clean (processed) body content, as well as header data.
    """
    message = email.message_from_file(mime_file)
    body = ''
    for part in message.walk():
        body += process_multipart(part)
    body = process_text(body)
    return dict((key, val) for key, val in message.items()), body


def preprocess(data_dir, file_range, outfname):
    """
    Converts the data from each file in the given range into a single string
    of words extracted out of the message body. The words are pre-filtered
    to remove symbols, numbers, and other filler content.
    The processed data is compiled into a single ARFF file, which can then
    be further preprocessed and converted into a bag-of-words format using
    Weka's filter tools.
    """
    label_file = open(data_dir + '/full/index', 'r')
    labels = label_file.readlines()
    label_file.close()
    data_dir = data_dir + '/data'
    messages = []
    for num in range(file_range[0], file_range[1]+1):
        label = labels[num-1].split()[0]
        fname = data_dir + '/inmail.' + str(num)
        mime_file = open(fname, 'r')
        header, body = process_message(mime_file)
        mime_file.close()
        messages.append((body, label))
    outfile = open(outfname, 'w')
    outfile.write("% ARFF generated by Python preprocessing script.\n\n")
    outfile.write("@RELATION email\n\n")
    outfile.write("@ATTRIBUTE body STRING\n")
    outfile.write("@ATTRIBUTE spam_or_ham_class {spam,ham}\n\n")
    outfile.write("@DATA\n")
    for pair in messages:
        message = pair[0]
        label = pair[1]
        outfile.write("\"" + message + "\", " + label + "\n")
    outfile.close()


def load_stopwords(fname):
    """
    Reads the file of the given filename and saves all the stopwords which
    will be used in the filtering process to remove them from the text.
    All stopwords will be converted to lowercase if they are not already.
    """
    stopwords_file = open(fname, 'r')
    for line in stopwords_file:
        stopword = line.strip()
        if (len(stopword) > 0) and (stopword[0] != '#'):
            STOPWORDS.append(stopword.lower())
    stopwords_file.close()


# Process args and run the preprocessing code.
if __name__ == '__main__':
    if len(sys.argv) < 5:
        print "Usage: <data_dir> <range_start> <range_end> <outfile> [stopword file]"
        exit(0)
    data_dir = sys.argv[1]
    range_start = sys.argv[2]
    range_end = sys.argv[3]
    outfname = sys.argv[4]
    if len(sys.argv) > 5:
        stopwords_file = sys.argv[5]
        load_stopwords(stopwords_file)
    file_range = (int(range_start), int(range_end))
    preprocess(data_dir, file_range, outfname)
