# Python script to convert the raw email data into .arff format for Weka.

import sys
import argparse
import re
import email


# Fill this list using load_stopwords() function.
STOPWORDS = []

# Regex to match any number (int or float) within a string.
FLOAT_REGEX = r'[+-]?(\d+(\.\d*)?|\.\d+)([eE][+-]?\d+)?'

# Range of allowable ASCII characters (rest are ignored).
ASCII_MIN = 32
ASCII_MAX = 126

# Alls symbols to be ignored by the filters.
SYMBOLS = [
    '~', '`', '!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '+',
    '=', '{', '}', '[', ']', '\\', '|', ':', ';', '"', '\'', ',', '.', '?', '/'
]


def filter_words(words):
    """
    Removes all unwanted symbols from each word, and splits words around
    unwanted delimiters. Also removes digits. After all filtering is done,
    stopwords are also removed (assuming stopwords list is filled).
    NOTE: The words are NOT converted into lowercase in the final output.
    This is a word-by-word preprocessing step.
    """
    filtered = []
    for word in words:
        # Replace digits with arbitrary symbol, and split around it later
        word = re.sub(FLOAT_REGEX, SYMBOLS[0], word)
        split = False
        for symbol in SYMBOLS:
            if symbol in word:
                parts = word.split(symbol)
                words.extend(parts)
                split = True
                break
        if (not split) and (len(word) > 0) and (word.lower() not in STOPWORDS):
            filtered.append(word)
    return filtered


def process_text(text):
    """
    Processes a single string of text and returns an untagged version of it.
    That is, removes any HTML tags, and any content contained inside the tags,
    and returns the string as raw words.
    This is a character-by-character preprocessing step that also calls the
    word-for-word preprocessing.
    """
    # remove HTML tags and non-ascii characters:
    words = text.split()
    intag = False
    raw_words = []
    for word in words:
        untagged = ''
        for ch in word:
            if ch == '<':
                intag = True
            elif ch == '>':
                intag = False
            elif (not intag) and (ASCII_MIN <= ord(ch) <= ASCII_MAX):
                untagged += ch
        if len(untagged) > 0:
            raw_words.append(untagged)
    # remove stopwords, numbers, and symbols:
    raw_words = filter_words(raw_words)
    raw_text = ' '.join(raw_words)
    return raw_text


def process_multipart(part):
    """Recursively processes a part of the message body content."""
    if type(part) is str:
        return part
    maintype = part.get_content_maintype()
    if maintype == 'text':
        return part.get_payload()
    elif maintype == 'multipart':
        text = ''
        for sub_part in part.get_payload():
            text += process_multipart(sub_part)
        return text
    else:
        return ''


def process_message(mime_file):
    """
    Separately processes the email stored in the provided MIME file, and
    returns the clean (processed) body content, as well as header data.
    """
    message = email.message_from_file(mime_file)
    body = ''
    for part in message.walk():
        body += process_multipart(part)
    body = process_text(body)
    return dict((key, val) for key, val in message.items()), body


def output_arff_file(messages, fname):
    """Writes the message and label pairs to an ARFF file."""
    outfile = open(fname, 'w')
    outfile.write("% ARFF generated by Python preprocessing script.\n\n")
    outfile.write("@RELATION email\n\n")
    outfile.write("@ATTRIBUTE body STRING\n")
    outfile.write("@ATTRIBUTE spam_or_ham_class {spam,ham}\n\n")
    outfile.write("@DATA\n")
    for pair in messages:
        message = pair[0]
        label = pair[1]
        outfile.write("\"" + message + "\", " + label + "\n")
    outfile.close()


def output_ngram_files(messages, fname, to_chars, to_lower):
    """
    Writes the messages to two files: one for ham emails and another for spam
    messages, depending on their paired labels. If parameter "chars" is true,
    the words are also going to be split up into space-delimited characters
    before being written out to the output file.
    """
    ham_messages = []
    spam_messages = []
    for pair in messages:
        message = pair[0]
        label = pair[1]
        if label == 'ham':
            ham_messages.append(message)
        else:
            spam_messages.append(message)
    if to_lower:
        ham_messages = map(str.lower, ham_messages)
        spam_messages = map(str.lower, spam_messages)
    if to_chars:
        categories = []
        for subset in [ham_messages, spam_messages]:
            temp = map(list, subset)
            category = []
            for message in temp:
                category.append(' '.join([ch for ch in message if ch != ' ']))
            categories.append(category)
        ham_messages = categories[0]
        spam_messages = categories[1]
    for category in [('ham', ham_messages), ('spam', spam_messages)]:
        outfile = open(category[0] + "_" + fname, 'w')
        for message in category[1]:
            outfile.write(message + "\n")
        outfile.close()


def preprocess(args):
    """
    Converts the data from each file in the given range into a single string
    of words extracted out of the message body. The words are pre-filtered
    to remove symbols, numbers, and other filler content.
    The processed data is compiled into a single ARFF file, which can then
    be further preprocessed and converted into a bag-of-words format using
    Weka's filter tools.
    """
    range_start = args.range_start
    range_end = args.range_end
    file_range = (int(range_start), int(range_end))
    label_file = open(args.data_dir + '/full/index', 'r')
    labels = label_file.readlines()
    label_file.close()
    data_dir = args.data_dir + '/data'
    messages = []
    for num in range(file_range[0], file_range[1]+1):
        label = labels[num-1].split()[0]
        fname = data_dir + '/inmail.' + str(num)
        mime_file = open(fname, 'r')
        header, body = process_message(mime_file)
        mime_file.close()
        messages.append((body, label))
    if args.to_ngrams:
        output_ngram_files(messages, args.outfile, args.ngram_chars, args.ngram_lower)
    else:
        output_arff_file(messages, args.outfile)


def load_stopwords(fname):
    """
    Reads the file of the given filename and saves all the stopwords which
    will be used in the filtering process to remove them from the text.
    All stopwords will be converted to lowercase if they are not already.
    """
    stopwords_file = open(fname, 'r')
    for line in stopwords_file:
        stopword = line.strip()
        if (len(stopword) > 0) and (stopword[0] != '#'):
            STOPWORDS.append(stopword.lower())
    stopwords_file.close()


# Process args and run the preprocessing code.
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('data_dir', \
                        help="Trec 2007 corpus directory.")
    parser.add_argument('range_start', type=int, \
                        help="First file in the data batch.")
    parser.add_argument('range_end', type=int, \
                        help="Last file in the data batch.")
    parser.add_argument('outfile', \
                        help="Output .arff file.")
    parser.add_argument('-stopwords', '--swfile', required=False, \
                        help="File of line-separated stopwords.")
    parser.add_argument('--ngrams', dest='to_ngrams', action='store_true', \
                        help="Set to true to output N-Gram format instead.")
    parser.add_argument('--ngram-chars', dest='ngram_chars', action='store_true', \
                        help="Use N-Gram characters instead of words.")
    parser.add_argument('--ngram-lower', dest='ngram_lower', action='store_true', \
                        help="Use only lowercase N-Grams.")
    args = parser.parse_args()
    if (not args.to_ngrams) and (args.swfile):
        load_stopwords(args.swfile)
    preprocess(args)
